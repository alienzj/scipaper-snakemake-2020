\documentclass{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{authblk}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{enumitem}
\setlist[enumerate]{itemsep=0mm}
\setlist[itemie]{itemsep=0mm}
\usepackage{xr}
\externaldocument{supplement.tex}

\usepackage[backend=biber]{biblatex}
\addbibresource{literature.bib}

\newcommand{\sharedfirst}{$\dagger$}
\newcommand{\sharedfirsttext}[1]{\affil[\sharedfirst]{#1}}
\newcommand{\corresponding}{*}
\newcommand{\correspondingtext}[1]{\affil[\corresponding]{#1}}
\newcommand{\image}[1]{\centering\includegraphics[width=\textwidth]{#1}}
\let\plainurl\url
\renewcommand{\url}[1]{\protect\plainurl{#1}}

\begin{document}

\author[1,2,\sharedfirst]{Felix Mölder}
\author[3,\sharedfirst]{Soo Lee}
\author[4,\sharedfirst]{Michael Hall}
\author[4,\sharedfirst]{Brice Letcher}
\author[5,\sharedfirst]{Vanessa Sochat}
\author[6,\sharedfirst]{Kim Philip Jablonski}
\author[1,7,\corresponding]{Johannes Köster}
\affil[1]{Algorithms for reproducible bioinformatics, Genome Informatics, Institute of Human Genetics, University Hospital Essen, University of Duisburg-Essen, Essen, Germany}
\affil[2]{Institute of Pathology, University Hospital Essen, University of Duisburg-Essen, Essen, Germany}
\affil[3]{Biomedical Informatics, Harvard Medical School, Harvard University, Boston, USA}
\affil[4]{EMBL-EBI}
\affil[5]{Stanford Computing, Stanford University}
\affil[6]{ETH Zürich}
\affil[7]{Medical Oncology, Harvard Medical School, Harvard University, Boston, USA}
\sharedfirsttext{Shared first author}
\correspondingtext{To whom correspondence should be addressed}

\title{Sustainable data analysis with Snakemake}
\maketitle

\begin{abstract}
	% abstract text
\end{abstract}

Performing data analysis has become ubiquitous across scientific disciplines.
Along with that, securing data analysis reproducibility has been identified as a major challenge~\parencite{Mesirov2010,Baker2016,Munaf__2017}.
In consequence, recent years have seen a wide adoption of scientific workflow management systems by the community.
Countless workflow management systems have been published (see~\url{https://github.com/pditommaso/awesome-pipeline}).
Roughly spoken, these can be partitioned into four niches, for which we will highlight the major representatives below.~ 

First, workflow management systems like Galaxy~\parencite{Afgan2018} offer a graphical user interface for composition and execution of workflows.
The obvious advantage is the shallow learning curve, making such systems accessible for everybody, without the need for programming skills.

Second, with systems like Anduril~\parencite{Cervera2019}, Balsam~\parencite{papka2018}, Hyperloom~\parencite{cima2018hyperloom}, Jug~\parencite{Coelho_2017}, Pwrake~\parencite{Tanaka_2010}, Ruffus~\parencite{Goodstadt2010}, SciPipe~\parencite{Lampa2019}, SCOOP \parencite{SCOOP_XSEDE2014}, and COMPSs~\parencite{Lordan_2013} workflows are specified using~ a set of classes and functions for generic programming languages like Python, Scala and others.
Such systems have the advantage that they can be used without a graphical interface (e.g. in a server environment), and that workflows can be straighforwardly managed with version control systems like Git (\url{https://git-scm.com}).~ 

Third, with systems like Nextflow~\parencite{Di_Tommaso_2017}, Snakemake~\parencite{Köster2012}, BioQueue~\parencite{Yao2017}, Bpipe~\parencite{Sadedin2012}, ClusterFlow~\parencite{Ewels2016}, Cylc~\parencite{J_Oliver_2018},~and BigDataScript~\parencite{Cingolani_2014}, workflows are specified using a domain specific language (DSL).
Here, the advantages of the second niche are shared, while adding the additional benefit of improved readability since the DSL provides statements and declarations that specifically model central components of workflow management, thereby obviating superfluous operators or boilerplate code.
In case of Nextflow and Snakemake, where the DSL is implemented as an extension to a generic programming language (Groovy and Python), even access to the full power of the underlying programming language is maintained (e.g. for implementing conditional execution and handling configuration).

Fourth, with systems like Popper~\parencite{Jimenez_2017}, workflow specification happens in a purely declarative way via configuration file formats like YAML.
Here, most of the benefits of the third niche are shared.
In addition, workflow specification can be particularly readable for non developers.
This comes however wit the downside of being more restricted since the facilities of imperative or functional programming are not available.

Fifth, there are system-independent workflow specification languages like CWL~\parencite{cwl} and WDL~\parencite{voss_full-stack_2017}.
These define a (declarative) syntax for specifying workflows, which can be parsed and executed by arbitrary executors, e.g. Cromwell (\url{https://cromwell.readthedocs.io}), Toil~\parencite{Vivian_2017} and Tibanna~\parencite{Lee_2019}.
Here, a main advantage is that the same workflow definition can be executed on various specialized execution backends, thereby promising scalability to virtually any computing platform.

Today, several of the above mentioned systems support full in silico reproducibility of data analyses (e.g. Galaxy, Nextflow, Snakemake, WDL, CWL), via allowing the definition and automatic scalable execution of each involved step, together with the ability to define and automatically deploy the software stack needed for each step (e.g. via the Conda package manager,~\url{https://docs.conda.io}, or Docker containers~\url{https://www.docker.com}).

Reproducibility is important to generate trust in scientific results.
However, we postulate that a truly sustainable data analysis needs to consider a full hierarchy of interdependent aspects (see Fig.~\ref{fig:sustainability}).

\begin{figure}
	\image{sustainability-in-wms.pdf}
	\caption{
		Hierarchy of aspects to consider for sustainable data analysis.
		By supporting the top layer, a workflow management system can promote the center layer, and thereby help to obtain true sustainability.
	}\label{fig:sustainability}
\end{figure}

By being \emph{automated}, \emph{scalable} to various computational platforms and levels of parallelism, and \emph{portable}, in the sense that it is able to be automatically deployed with all required software in exactly the needed versions, a data analysis gains full in silico \emph{reproducibility.
}

While being able to reproduce results is a major achievement, \emph{transparency} is equally important: the validity of results can only be fully assessed if~the parameters, software and custom code of each analysis step is fully accessible.
On the level of the code, a data analysis therefore has to be \emph{readable} and well \emph{documented}.
On the level of the results it has to be possible to \emph{trace} parameters, software stack and code through all involved steps.

Finally, valid results yielded from a reproducible data analysis become even more beneficial for the scientific community once the analysis can be reused for other projects.
In practice, this will almost never be a plain reuse, and instead requires~\emph{adaptability} to new circumstances, e.g. being able to extend the analysis, replace or modify steps and adjust parameter choices.
Such adaptability can only be achieved if the data analysis can easily be executed at a different computational environment (e.g. a different institute), thus it has to be \emph{scalable} and \emph{portable} again.
In addition, it is crucial that the analysis code is as \emph{readable} as possible such that it can be easily modified.

In this work, we show how sustainability in terms of these aspects is supported by Snakemake.
Since its original publication in 2012, Snakemake has gained a wide adoption, culminating in, on average, nowadays more than 3 new citations per week, and over 600 citations in total (Fig.~\ref{fig:citations}), making it one of the most widely used workflow management systems in science.

\begin{figure}
	\image{citations.pdf}
	\caption{
		Citations of the original Snakemake article, (a) by year, (b) by scientific discipline of the citing article.
		Data source:~\url{https://badge.dimensions.ai/details/id/pub.1018944052}, 2020/05/20.
	}
	\label{fig:citations}
\end{figure}

We first present several central and novel contributions to the field implemented in Snakemake.
Second, we show how Snakemake comprehensively covers data analysis needs by introducing generic workflow design patterns that can serve as blueprints for composing any kind of analysis.

\section{Results}

In the following, we describe how sustainability is achieved with Snakemake by following the central goals outlined above.

\subsection{Automation}

The key idea of Snakemake is that workflows are specified by decomposing them into steps, which are represented as~\emph{rules~}(Fig.~\ref{fig:example}).
Each rule describes how to obtain a set of output files from a set of input files.
This can happen via a shell command, a block of Python code, an external script (Python, R, or Julia), a Jupyter notebook (\url{https://jupyter.org}), or a so-called tool wrapper (see Sec.~\ref{sec:wrappers}).
By the use of wildcards, rules can be generic.
For example, see the rule~\lstinline!select_by_country! in Fig.~\ref{fig:example}a.
It can be applied to generate any output file of the form~\lstinline!by-country/{country}.csv!, with~\lstinline!{country}! being a wildcard that can be replaced with any non-empty string.
In shell commands, input, and output files, as well as additional parameters are directly accessible by enclosing the respective keywords in curly braces (in case of more than a single item in any of these, access can happen by name or index).

\begin{figure}
	\image{example-workflow.pdf}
	\caption{
		Example Snakemake workflow. (a) workflow definition; hypothesized knowledge requirement for line readability is color-coded on the left next to the line numbers. (b) directed acyclic graph (DAG) of jobs, representing the automatically derived execution plan from the example workflow; job node colors reflect rule colors in the workflow definition. (c) content of script plot-hist.py referred from rule plot\_histogram. (d) knowledge requirements for readability by statement category.
	}
	\label{fig:example}
\end{figure}

When using script integration instead of shell commands, Snakemake automatically inserts an object giving access to all properties of the job (e.g. \lstinline!snakemake.output[0]!, see Fig.
\ref{example}c).
This way, the boiler plate code for parsing command line arguments can be avoided.

By replacing wildcards with concrete values, Snakemake turns any rule into a job which will be executed in order to generate the defined output files.
Dependencies between jobs are implicit, and inferred automatically in the following way.
For each input file of a job, Snakemake determines a rule that can generate it, for example by replacing wildcards again (ambiguity can be resolved by prioritization or constraining wildcards) yielding another job.
Then Snakemake goes on recursively for the latter, until all input files of all jobs are either generated by another job or already present in the used storage (e.g. on disk).

\subsection{Readability}

The workflow definition language of Snakemake is designed to allow maximum readability, which is crucial for transparency and adaptability.
For natural language readability, the occurrence of known words is important.~For example, the Dale-Chall readability formula derives a score from the fraction of potentially unknown words (which do not occur in a list of common words) among all words in a text~\parencite{chall_readability_1995}.
For workflow definition languages, one has to additionally consider whether punctuation and operator usage is intuitively understandable.
When analyzing above example workflow (Fig.~\ref{fig:example}a) under these aspects, code statements fall into seven categories: 

\begin{enumerate}
	\item a natural language word, followed by a colon (e.g.~\lstinline!input:! and~\lstinline!output:!),
	\item the word ``rule'', followed by a name and a colon (e.g. \lstinline!rule\ convert\_to\_pdf:!),
	\item a quoted filename pattern (e.g. \lstinline!"{prefix}.pdf"!),
	\item a quoted shell command,
	\item a quoted wrapper identifier,
	\item a quoted container URL
	\item a Python statement.
\end{enumerate}

\printbibliography
\end{document}
