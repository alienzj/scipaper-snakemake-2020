\documentclass[parskip=half]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{authblk}
\usepackage{xr}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{enumitem}
\setlist[enumerate]{itemsep=0mm}
\setlist[itemize]{itemsep=0mm}

\externaldocument[s-]{supplement}

\usepackage[backend=biber,maxbibnames=999]{biblatex}
\addbibresource{literature.bib}

\newcommand{\sharedfirst}{$\dagger$}
\newcommand{\sharedfirsttext}[1]{\affil[\sharedfirst]{#1}}
\newcommand{\corresponding}{*}
\newcommand{\correspondingtext}[1]{\affil[\corresponding]{#1}}
\newcommand{\image}[1]{\centering\includegraphics[width=\textwidth]{#1}}
\let\plainurl\url
\renewcommand{\url}[1]{\protect\plainurl{#1}}

\begin{document}

\author[1,2,\sharedfirst]{Felix Mölder}
\author[3,\sharedfirst]{Soo Lee}
\author[4,\sharedfirst]{Michael Hall}
\author[4,\sharedfirst]{Brice Letcher}
\author[5,\sharedfirst]{Vanessa Sochat}
\author[6,\sharedfirst]{Kim Philip Jablonski}
\author[1,7,\corresponding]{Johannes Köster}
\affil[1]{Algorithms for reproducible bioinformatics, Genome Informatics, Institute of Human Genetics, University Hospital Essen, University of Duisburg-Essen, Essen, Germany}
\affil[2]{Institute of Pathology, University Hospital Essen, University of Duisburg-Essen, Essen, Germany}
\affil[3]{Biomedical Informatics, Harvard Medical School, Harvard University, Boston, USA}
\affil[4]{EMBL-EBI}
\affil[5]{Stanford Computing, Stanford University}
\affil[6]{ETH Zürich}
\affil[7]{Medical Oncology, Harvard Medical School, Harvard University, Boston, USA}
\sharedfirsttext{Shared first author}
\correspondingtext{To whom correspondence should be addressed}

\title{Sustainable data analysis with Snakemake}
\maketitle

\begin{abstract}
	% abstract text
\end{abstract}

Performing data analysis has become ubiquitous across scientific disciplines.
Along with that, securing data analysis reproducibility has been identified as a major challenge~\parencite{Mesirov2010,Baker2016,Munaf__2017}.
In consequence, recent years have seen a wide adoption of scientific workflow management systems by the community.
Countless workflow management systems have been published (see~\url{https://github.com/pditommaso/awesome-pipeline}).
Roughly spoken, these can be partitioned into four niches, for which we will highlight the major representatives below.~ 

First, workflow management systems like Galaxy~\parencite{Afgan2018} offer a graphical user interface for composition and execution of workflows.
The obvious advantage is the shallow learning curve, making such systems accessible for everybody, without the need for programming skills.

Second, with systems like Anduril~\parencite{Cervera2019}, Balsam~\parencite{papka2018}, Hyperloom~\parencite{cima2018hyperloom}, Jug~\parencite{Coelho_2017}, Pwrake~\parencite{Tanaka_2010}, Ruffus~\parencite{Goodstadt2010}, SciPipe~\parencite{Lampa2019}, SCOOP \parencite{SCOOP_XSEDE2014}, and COMPSs~\parencite{Lordan_2013} workflows are specified using~ a set of classes and functions for generic programming languages like Python, Scala and others.
Such systems have the advantage that they can be used without a graphical interface (e.g. in a server environment), and that workflows can be straighforwardly managed with version control systems like Git (\url{https://git-scm.com}).~ 

Third, with systems like Nextflow~\parencite{Di_Tommaso_2017}, Snakemake~\parencite{Köster2012}, BioQueue~\parencite{Yao2017}, Bpipe~\parencite{Sadedin2012}, ClusterFlow~\parencite{Ewels2016}, Cylc~\parencite{J_Oliver_2018},~and BigDataScript~\parencite{Cingolani_2014}, workflows are specified using a domain specific language (DSL).
Here, the advantages of the second niche are shared, while adding the additional benefit of improved readability since the DSL provides statements and declarations that specifically model central components of workflow management, thereby obviating superfluous operators or boilerplate code.
In case of Nextflow and Snakemake, where the DSL is implemented as an extension to a generic programming language (Groovy and Python), even access to the full power of the underlying programming language is maintained (e.g. for implementing conditional execution and handling configuration).

Fourth, with systems like Popper~\parencite{Jimenez_2017}, workflow specification happens in a purely declarative way via configuration file formats like YAML.
Here, most of the benefits of the third niche are shared.
In addition, workflow specification can be particularly readable for non developers.
This comes however wit the downside of being more restricted since the facilities of imperative or functional programming are not available.

Fifth, there are system-independent workflow specification languages like CWL~\parencite{cwl} and WDL~\parencite{voss_full-stack_2017}.
These define a (declarative) syntax for specifying workflows, which can be parsed and executed by arbitrary executors, e.g. Cromwell (\url{https://cromwell.readthedocs.io}), Toil~\parencite{Vivian_2017} and Tibanna~\parencite{Lee_2019}.
Here, a main advantage is that the same workflow definition can be executed on various specialized execution backends, thereby promising scalability to virtually any computing platform.

Today, several of the above mentioned systems support full in silico reproducibility of data analyses (e.g. Galaxy, Nextflow, Snakemake, WDL, CWL), via allowing the definition and automatic scalable execution of each involved step, together with the ability to define and automatically deploy the software stack needed for each step (e.g. via the Conda package manager,~\url{https://docs.conda.io}, or Docker containers~\url{https://www.docker.com}).

Reproducibility is important to generate trust in scientific results.
However, we postulate that a truly sustainable data analysis needs to consider a full hierarchy of interdependent aspects (see Fig.~\ref{fig:sustainability}).

\begin{figure}
	\image{sustainability-in-wms.pdf}
	\caption{
		Hierarchy of aspects to consider for sustainable data analysis.
		By supporting the top layer, a workflow management system can promote the center layer, and thereby help to obtain true sustainability.
	}\label{fig:sustainability}
\end{figure}

By being \emph{automated}, \emph{scalable} to various computational platforms and levels of parallelism, and \emph{portable}, in the sense that it is able to be automatically deployed with all required software in exactly the needed versions, a data analysis gains full in silico \emph{reproducibility.
}

While being able to reproduce results is a major achievement, \emph{transparency} is equally important: the validity of results can only be fully assessed if~the parameters, software and custom code of each analysis step is fully accessible.
On the level of the code, a data analysis therefore has to be \emph{readable} and well \emph{documented}.
On the level of the results it has to be possible to \emph{trace} parameters, software stack and code through all involved steps.

Finally, valid results yielded from a reproducible data analysis become even more beneficial for the scientific community once the analysis can be reused for other projects.
In practice, this will almost never be a plain reuse, and instead requires~\emph{adaptability} to new circumstances, e.g. being able to extend the analysis, replace or modify steps and adjust parameter choices.
Such adaptability can only be achieved if the data analysis can easily be executed at a different computational environment (e.g. a different institute), thus it has to be \emph{scalable} and \emph{portable} again.
In addition, it is crucial that the analysis code is as \emph{readable} as possible such that it can be easily modified.

In this work, we show how sustainability in terms of these aspects is supported by Snakemake.
Since its original publication in 2012, Snakemake has gained a wide adoption, culminating in, on average, nowadays more than 3 new citations per week, and over 600 citations in total (Fig.~\ref{fig:citations}), making it one of the most widely used workflow management systems in science.

\begin{figure}
	\image{citations.pdf}
	\caption{
		Citations of the original Snakemake article, (a) by year, (b) by scientific discipline of the citing article.
		Data source:~\url{https://badge.dimensions.ai/details/id/pub.1018944052}, 2020/05/20.
	}
	\label{fig:citations}
\end{figure}

We first present several central and novel contributions to the field implemented in Snakemake.
Second, we show how Snakemake comprehensively covers data analysis needs by introducing generic workflow design patterns that can serve as blueprints for composing any kind of analysis.

\section{Results}

In the following, we describe how sustainability is achieved with Snakemake by following the central goals outlined above.

\subsection{Automation}

The key idea of Snakemake is that workflows are specified by decomposing them into steps, which are represented as~\emph{rules~}(Fig.~\ref{fig:example}).
Each rule describes how to obtain a set of output files from a set of input files.
This can happen via a shell command, a block of Python code, an external script (Python, R, or Julia), a Jupyter notebook (\url{https://jupyter.org}), or a so-called tool wrapper (see Sec.~\ref{sec:wrappers}).
By the use of wildcards, rules can be generic.
For example, see the rule~\lstinline!select_by_country! in Fig.~\ref{fig:example}a.
It can be applied to generate any output file of the form~\lstinline!by-country/{country}.csv!, with~\lstinline!{country}! being a wildcard that can be replaced with any non-empty string.
In shell commands, input, and output files, as well as additional parameters are directly accessible by enclosing the respective keywords in curly braces (in case of more than a single item in any of these, access can happen by name or index).

\begin{figure}
	\image{example-workflow.pdf}
	\caption{
		Example Snakemake workflow. (a) workflow definition; hypothesized knowledge requirement for line readability is color-coded on the left next to the line numbers. (b) directed acyclic graph (DAG) of jobs, representing the automatically derived execution plan from the example workflow; job node colors reflect rule colors in the workflow definition. (c) content of script plot-hist.py referred from rule plot\_histogram. (d) knowledge requirements for readability by statement category.
	}
	\label{fig:example}
\end{figure}

When using script integration instead of shell commands, Snakemake automatically inserts an object giving access to all properties of the job (e.g. \lstinline!snakemake.output[0]!, see Fig.
\ref{example}c).
This way, the boiler plate code for parsing command line arguments can be avoided.

By replacing wildcards with concrete values, Snakemake turns any rule into a job which will be executed in order to generate the defined output files.
Dependencies between jobs are implicit, and inferred automatically in the following way.
For each input file of a job, Snakemake determines a rule that can generate it, for example by replacing wildcards again (ambiguity can be resolved by prioritization or constraining wildcards) yielding another job.
Then Snakemake goes on recursively for the latter, until all input files of all jobs are either generated by another job or already present in the used storage (e.g. on disk).

\subsection{Readability}

The workflow definition language of Snakemake is designed to allow maximum readability, which is crucial for transparency and adaptability.
For natural language readability, the occurrence of known words is important.~For example, the Dale-Chall readability formula derives a score from the fraction of potentially unknown words (which do not occur in a list of common words) among all words in a text~\parencite{chall_readability_1995}.
For workflow definition languages, one has to additionally consider whether punctuation and operator usage is intuitively understandable.
When analyzing above example workflow (Fig.~\ref{fig:example}a) under these aspects, code statements fall into seven categories (\autoref{s-sec:readability}).
In addition, for each line, we can judge whether it needs 

\begin{enumerate}
	\item domain knowledge (from the field analyzed in the given workflow),
	\item technical knowledge (e.g. about Unix-style shell commands or Python),
	\item Snakemake knowledge,
	\item general education (e.g. they should be understandable for everybody).
\end{enumerate}

In Fig.~{\ref{fig:example}}, we hypothesize the required knowledge for readability of each code line.
Most statements are understandable with either general education, domain, or technical knowledge.
In particular, only five lines need specific Snakemake knowledge (Fig.~{\ref{fig:example}}d).
The rationale for each hypothesis can be found in supplementary \autoref{s-sec:readability}.

While this example is obviously not as evolved as real world data analyses, the ratio of Snakemake knowledge lines and lines that are readable with general education, domain or technical knowledge can be expected to stay roughly the same.
Since Snakemake supports modularization of workflow definitions, it is moreover possible to hide away more technical parts of the workflow definition (e.g. helper functions or variables), in order to not distract the reader from understanding the main steps of the data analysis.

Since dependencies between jobs are implicitly encoded via matching filename patterns, we hypothesize that in general no specific technical knowledge is necessary to understand the connections between the rules.
Instead, it should be quite intuitive to conclude that an input of one rule that reoccurs as an output of another reflects a dependency.

\subsubsection{Jupyter notebook integration}

\subsubsection{Tool wrappers}\label{sec:wrappers}

\subsubsection{Standardized code linting and formatting}

\subsection{Portability}

\subsubsection{Conda integration}

\subsubsection{Container integration}

\subsection{Traceability and documentation}

\subsection{Scalability}

\subsubsection{Job scheduling}

Naturally, due to their dependencies, not all jobs in a workflow can be executed at the same time.
Instead, one can imagine two cut lines, that partition the DAG of jobs into those that are already finished, those that have already been scheduled but are not finished yet, and those that have not yet been scheduled.
Within the latter partition, all jobs that have no incoming edge from the same partition can be scheduled next.
We call this the set $J$ of pending jobs.
The scheduling problem a workflow manager like Snakemake has to solve is to select the subset $E \subseteq J$ that leads to an optimal execution of the workflow, while not exceeding given computational resources.
Optimality of execution is thereby fueled by three criteria.
First, it should be as fast as possible.
Second, high priority jobs should be preferred (Snakemake allows prioritization of jobs via the workflow definition and the command line).
Third, temporary output files should be deleted as fast as possible (Snakemake allows to mark output files as temporary, which leads to their automatic deletion once all consuming jobs have been finished).

\newcommand{\N}{\mathbb{N}}

Let $R$ be the set of resources used in the workflow.
For each job $j \in J$, let $p_j \in \N$ be its priority, let $u_{r,j} \in \N$ be its usage of resource $r \in R$.
Further, let $U_r$ be the free capacity of resource $r \in R$ (initially what is provided to Snakemake at the command line, later what is left given already running resources).
\\
Let $F$ be the set of all temporary output files in the workflow, let $n_{f,j} \in \{0,1\}$ be an indicator variable denoting whether job $j \in J$ in consumes the temporary file $f$, and let $s_f$ be the size of file $f \in F$.

Then, the scheduling problem can be written as linear optimization problem 

\begin{align*}
	\text{minimize} & T \cdot S \cdot \sum_{j \in J} x_j \cdot P_j + S \cdot \sum_{j \in J} x_j \cdot (T_j+1) + \sum_{f \in F} i_f \cdot S_f \\ \text{subject to} & x_j \in \{0,1\}\\ \quad & i_j \in [0,1]
\end{align*}

\subsubsection{Between workflow caching} 

While data analysis usually entails the handling of a set of datasets or samples that are specific to a particular project, it often additionally relies on a set of steps that retrieve and post-processes some common datasets.
For example, in the life sciences, such datasets are reference genomes and corresponding annotations.
Since such datasets potentially reoccur in many analyses conducted in a lab or institute, re-executing the corresponding analysis steps for retrieval and post-processing thereof for each individual data analysis would waste both disk space and computation time.

Historically, the solution in practice was to build up shared resources with the postprocessed datasets that could then be referrred to from the workflow definition.
For example, in the life sciences, this has led to the Illumina iGenomes resource (\url{https://support.illumina.com/sequencing/sequencing\_software/igenome.html}) and the GATK resource bundle (\url{https://gatk.broadinstitute.org/hc/en-us/articles/360035890811-Resource-bundle}).
In addition, in order to provide a more flexible way of selection and retrieval for such shared resources, so-called reference management systems have been published, like Go Get Data (\url{https://gogetdata.github.io}) and RefGenie (\url{http://refgenie.databio.org}).
Here, the logic for retrieval and post-processing is curated in a set of recipes or scripts, and the resulting resources can be automatically retrieved via command line utilities.
The downside of all these approaches is that the transparency of the data analysis is hampered since the steps taken to obtain the used resources are hidden away and inaccessible without additional work for the reader of the data analysis.

Snakemake provides a new, generic approach to the problem which does not have this downside (see \autoref{fig:caching}).
Leveraging the workflow-inherent information, Snakemake can calculate a hash value for each job which unambiguously captures exactly how an output file is generated, prior to actually generating the file.
This hash can be used to store and lookup output files in a central cache (e.g. a folder on the same machine or in a remote storage).
Hence, for any output file in a workflow, if the corresponding rule is marked as eligible for caching, Snakemake can obtain the file from the cache if it has been created before in a different workflow or by a different user on the same system, thereby saving computation time, as well as disk space (the file can be linked instead of copied).

\begin{figure}
	\image{caching.pdf}
	\caption{
		Blockchain based between workflow caching scheme of Snakemake.
		If a job is eligible for caching, its code, parameters, raw input files, software environment and the hashes of its dependencies are used to calculate a SHA-256 hash value, under which the output files are stored in a central cache.
		Subsequent runs of the same job (with the same dependencies) in other workflows can skip the execution and directly take the output files from the cache.
	}
	\label{fig:caching}
\end{figure}

The hash value is calculated in the following way.
Let $J$ be the set of jobs of a workflow.
For any job $j \in J$, let $c_j$ denote its code (shell command, script, wrapper, or notebook), let $P_j = \{(k_i, v_i) \mid i=0,\dots,m\}$ be its set of parameters (with key $k_i$ and JSON-encoded value $v_i$), let $F_j$ be its set of input files that are not created by any other job, and let $s_j$ be a string describing the associated software environment (either a container URL, a conda environment definition, or both).
Then, assuming that job $j \in J$ with dependencies $D_j \subset J$ is the job of interest, we can calculate the hash value as $$ h(j) = h'\left( c_j \oplus \left(\bigoplus_{i=0}^m k_i \oplus v_i \right) \oplus \left( \bigoplus_{f \in F_j} h'(f) \right) \oplus s_j \oplus \left( \bigoplus_{j' \in D_j} h(j') \right) \right) $$ with $h'$ being the SHA256 hash function, $\oplus$ being the string concatenation, and $\bigoplus$ being the string concatenation of its operands in lexicographic order.

The hash function~\(h(j)\) hence comprehensively describes everything that affects the content of the output files of job~\(j\), namely code, parameters, raw input files, software environment and input generated by jobs it depends on.
For the latter, we recursively apply the hash function~\(h\) again.
In other words, for each dependency~\(j' \in D_j\) we include a hash value into the hash of job~\(j\), which is in fact the hashing principle behind blockchains.

\printbibliography
\end{document}
