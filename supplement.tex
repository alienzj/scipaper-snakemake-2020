\documentclass[10pt,a4paper,twocolumn]{article}
\usepackage{f1000_styles}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\lstset{basicstyle=\ttfamily}
\usepackage[hidelinks]{hyperref}
\usepackage{enumitem}
\setlist[enumerate]{itemsep=0mm}
\setlist[itemize]{itemsep=0mm}

% Cross referencing external documents.
\usepackage{xr}
% Command for registering file dependencies with latexmk.
\makeatletter
\newcommand*{\addFileDependency}[1]{% argument=file name and extension
  \typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother

\externaldocument[m-]{main}
\addFileDependency{main.tex}
\addFileDependency{main.aux}

\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thefigure}{S\arabic{figure}}

\newcommand{\image}[1]{\centering\includegraphics[width=\textwidth]{#1}}
\let\plainurl\url
\renewcommand{\url}[1]{\protect\plainurl{#1}}

\usepackage[backend=bibtex,maxbibnames=999]{biblatex}
\addbibresource{literature.bib}



\begin{document}

\title{Supplement:\\
Sustainable data analysis with Snakemake}
\maketitle
\thispagestyle{fancy}

\section{Readability}
\label{sec:readability}

Statements in Snakemake workflow definitions fall into seven categories:
\begin{enumerate}
	\item a natural language word, followed by a colon (e.g.~\lstinline!input:! and~\lstinline!output:!),
	\item the word ``rule'', followed by a name and a colon (e.g. \lstinline!rule convert_to_pdf:!),
	\item a quoted filename pattern (e.g. \lstinline!"{prefix}.pdf"!),
	\item a quoted shell command,
	\item a quoted wrapper identifier,
	\item a quoted container URL
	\item a Python statement.
\end{enumerate}

Below, we list the rationale of our assessment for each category in \autoref{m-fig:example}:

\begin{enumerate}
	\item The natural language word is either trivially understandable (e.g.\ \lstinline!input:! and \lstinline!output:!) or understandable with technical knowledge (\lstinline!container:! or~\lstinline!conda:!).
	      The colon straightforwardly shows that the content follows next.
	      Only for the wrapper directive (\lstinline!wrapper:!) one needs to have the Snakemake specific knowledge that it is possible to refer to publicly available tool wrappers.
	\item
	      The word ``rule'' is trivially understandable, and when
	      carefully choosing rule names, at most domain knowledge is needed for
	      understanding such statements.
	\item
	      Filename patterns can mostly be understood with domain knowledge,
	      since the file extensions should tell the expert what kind of content
	      will be used or created.
	      We hypothesize that wildcard definitions (e.g.~\lstinline!{country}!) are straightforwardly understandable as a placeholder.
	\item
	      Shell commands will usually need domain and technical knowledge for
	      understanding.
	\item
	      Wrapper identifiers can be understood with Snakemake knowledge only,
	      since one needs to know about the central tool wrapper repository of
	      Snakemake.
	      Nevertheless, with only domain knowledge one can at least conclude that the mentioned tool (last part of the wrapper ID) will be used in the wrapper.
	\item
	      A container URL will usually be understandable with technical
	      knowledge.
	\item
	      Python statements will either need technical knowledge or Snakemake
	      knowledge (when using the Snakemake API, as it happens here with the
	      expand command, which allows to aggregate over a combination of
	      wildcard values).
\end{enumerate}



\section{Design patterns}
\label{sec:design-patterns}

\autoref{fig:design-patterns} shows advanced design patterns which are less common but useful in certain situations.
For brevity only rule properties that are necessary to understand each example are shown (e.g. omitting log directives and shell commands).
Below, we explain each example in detail.

\begin{figure*}
	\image{design-patterns.pdf}
	\caption{Additional design patterns for Snakemake workflows.
		For brevity only rule properties that are necessary to understand each example are shown (e.g. omitting log directives and shell commands).
		(a) scatter/gather process, (b) streaming, (c) non-file parameters, (d) sample sheet based configuration, (e) conditional execution, (f) benchmarking.
		See \autoref{sec:design-patterns} for details.
	}\label{fig:design-patterns}
\end{figure*}

\paragraph{Scatter/gather processes (\autoref{fig:design-patterns}a).}
Snakemake's ability to employ arbitrary Python code for defining a rule's input and output files already enables any kind of scattering, gathering, and aggregations in workflows.
Nevertheless, it can be more readable and scalable to use Snakemake's explicit support for scatter/gather processes.
A Snakemake workflow can have any number of such processes, each of which has a name (here \lstinline!someprocess!).
In this example, the rule \lstinline!scatter! (line 4) splits some data into $n$ items; the rule \lstinline!step2! (line 8) is applied to each item; the rule \lstinline!gather! (line 14) aggregates over the outputs of \lstinline!step2! for each item.
Thereby, $n$ is defined via the \lstinline!scattergather! directive (line 1) at the beginning, which sets $n$ for each scatter/gather process in the workflow.
In addition, $n$ can be set via the command line via the flag \lstinline!--set-scatter!.
For example, here, we could set the number of scatter items to 16 by specifying \lstinline!--set-scatter someprocess=16!.
This enables the user to better scale the data analysis workflow to its computing platform, beyond the defaults provided by the workflow designer.

\paragraph{Streaming (\autoref{fig:design-patterns}b).}
Snakemake allows to stream output between jobs, instead of writing it to disk (see \autoref{m-sec:streaming} in the main document).
Here, the output of rule \lstinline!step1! (line 1) and \lstinline!step2! (line 7) is streamed into rule \lstinline!step3! (line 13).

\paragraph{Non-file parameters (\autoref{fig:design-patterns}c).}
Data analysis steps can need additional non-file input in the form of parameters, that are for example obtained from the workflow configuration (see \autoref{m-sec:automation} in the main document).
Both input files and such non-file parameters can optionally be defined via a Python function, which is evaluated for each job, when wildcard values are known.
In this example, we define a lambda expression (an anonymous function in Python), that retrieves a threshold depending on the value of the wildcard sample (\lstinline!w.sample!, line 7).
Wildcard values are passed as the first positional argument to such functions (here \lstinline!w!, line 7).

\paragraph{Iteration (\autoref{fig:design-patterns}d).}
Sometimes, a certain step in a data analysis workflow needs to be applied iteratively. Snakemake allows to model defining by setting the iteration count variable as a wildcard (here \lstinline!{i}!, line 16). Then, an input function can be used to either request the output of the previous iteration (if \lstinline!i > 0!, line 10) or the initial data (if \lstinline!i == 0!, line 8). Finally, in the rule that requests the final iteration result, the wildcard \lstinline!{i}! is set to the desired count (here \lstinline!10!, line 3).

\paragraph{Sample sheet based configuration (\autoref{fig:design-patterns}e).}
Often, scientific experiments entail multiple samples, for which meta-information is known (e.g. gender, tissue etc. in biomedicine).
Portable encapsulated projects (PEPs, \url{https://pep.databio.org}) are an approach to standardize such information and provide them in a shareable format.
Snakemake workflows can be directly integrated with PEPs, thereby allowing to configure them via meta-information that is contained in the sample sheets defined by the PEP.
Here, a pepfile (line 1) along with a validation schema (line 2) is defined, followed by an aggregation over all samples defined in the contained sample sheet.

\paragraph{Conditional execution (\autoref{fig:design-patterns}f).}
By default, Snakemake determines the entire DAG of jobs upfront, before the first job is executed.
However, sometimes the analysis path that shall be taken depends on some intermediate results.
For example, this is the case when filtering samples based on quality control criteria.
At the beginning of the data analysis, some quality control (QC) step is performed, which yields QC values for each sample.
The actual analysis that shall happen afterwards might be only suitable for samples that pass the QC.
Hence, one might have to filter out samples that do not pass the QC.
Since the QC is an intermediate result of the same data analysis, it can be necessary to determine the part of the DAG that comes downstream of the QC only after QC has been finalized.
Of course, one option is to separate QC and the actual analysis into two workflows, or defining a separate target rule for QC, such that it can be manually completed upfront, before the actual analysis is started.
Alternatively, if QC shall happen automatically as part of the whole workflow, one can make use of Snakemake's conditional execution capabilities.
In the example, we define that the \lstinline!qc! rule shall be a so-called \lstinline!checkpoint!.
Rules can depend on such checkpoints by obtaining their output from a global \lstinline!checkpoints! object (line 2), that is accessed inside of a function, which is passed to the \lstinline!input! directive of the rule (line 11).
This function is re-evaluated after the checkpoint has been executed (and its output files are present), thereby allowing to inspect the content of the checkoint's output files, and decide about the input files based on that.
In this example, the checkpoint rule \lstinline!qc! creates a TSV file, which the function loads, in order to extract only those samples for which the column \lstinline!"some-value"! contains a value greater than $90$ (line 6).
Only for those samples, the file \lstinline!"results/processed/{sample}.txt"! is requested, which is then generated by applying the rule \lstinline!process! for each of these samples.

\paragraph{Benchmarking (\autoref{fig:design-patterns}g).}
Sometimes, a data analysis entails the benchmarking of certain tools in terms of runtime, CPU, and memory consumption.
Snakemake directly supports such benchmarking by defining a \lstinline!benchmark! directive in a rule (line 7).
This directive takes a path to a TSV file.
Upon execution of a job spawned from such a rule, Snakemake will constantly measure CPU and memory consumption, and store averaged results together with runtime information into the given TSV file.
Benchmark files can be input to other rules, for example in order to generate plots or summary statistics.

\paragraph{Parameter space exploration (\autoref{fig:design-patterns}h).}
In Python (and therefore also with Snakemake), large parameter spaces can be represented very well via Pandas \parencite{reback2020pandas,mckinney-proc-scipy-2010} data frames.
When such a parameter space shall be explored by the application of a set of rules to each instance of the space (i.e., each row of the data frame), the idiomatic approach in Snakemake is to encode each data frame column as a wildcard and request all occuring combinations of values (i.e., the data frame rows), by some consuming rule.
However, with large parameter spaces that have a lot of columns, the wildcard expressions could become cumbersome to write down explicitly in the Snakefile.
Therefore, Snakemake provides a helper called \lstinline!Paramspace!, which can wrap a Pandas data frame (this functionality was inspired by the JUDI workflow management system \url{https://pyjudi.readthedocs.io}).
The helper allows to retrieve a wildcard pattern (via the property \lstinline!wildcard_pattern!) that encodes each column of the data frame in the form \lstinline!name~{name}! (i.e., column name followed by the wildcard/wildcard value).
The wildcard pattern can be formatted into input or output file names of rules (line 15).
The method \lstinline!instance! of the \lstinline!Paramspace! object, automatically returns the corresponding data frame row (as a Python \lstinline!dict!) for given wildcard values (here, that method is automatically evaluated by Snakemake for each instance of the rule \lstinline!simulate!, line 17).
Finally, aggregation over a parameter space becomes possible via the property \lstinline!instance_patterns!, which retrieves a concrete pattern of above form for each data frame row.
Using the \lstinline!expand! helper, these patterns can be formatted into a file path (line 8-11), thereby modelling an aggregation over the entire parameter space.
Naturally, filtering rows or columns on the paramspace via the usual Pandas methods allows to generate sub-spaces.



\section{Scheduling: Further Considerations}
\label{sec:morescheduling}

\begin{figure*}
 \centering
 \includegraphics[width=12cm]{benchmark.pdf}
 \caption{Runtime and memory usage of Snakemake while building the graph of jobs depending on the number of jobs in the workflow. The Snakemake workflow generating the results is available at \url{https://doi.org/10.5281/zenodo.4244143} and a self-contained Snakemake report with workflow code is available as supplementary file.}\label{fig:benchmark}
\end{figure*}

While the first releases of Snakemake used a greedy scheduler, the current implementation aims at using more efficient schedules by solving a mixed integer linear program (MILP) whenever there are free resources.
The current implementation already works well; still, future releases may consider additional objectives:
\begin{itemize}
\item The current formulation leads to fast removal of existing temporary files. In addition, one may control creation of temporary files in the first place, such that only limited space is occupied by temporary files at any time point during workflow execution.
\item It may also be beneficial to initially identify bottleneck jobs in the graph and prioritize them automatically instead of relying on the workflow author to prioritize them.
\end{itemize}
%%
Because we consider different objectives hierarchically and use large constants in the objective function, currently a high solver precision is needed.
If more objectives are considered in the future, an alternative hierarchical formulation may be used:
First find the optimal objective value for the first (or the first two) objectives; then solve another MILP that maximizes less important objectives and ensures via constraints that the optimality of the most important objective(s) is not violated, or stays within, say, 5\% of the optimal value.

\newcommand{\cores}{\text{c}}

We also need to mention a technical detail about the interaction between the scheduler and streams (\autoref{sec:design-patterns}).
Some jobs that take part in handling a data stream may effectively use zero cores (because they mostly wait for data and then only read or write data), i.e.\ they have $u_{\cores,j} = 0$ in the MILP notation, which means that they do not contribute to the objective function. 
We thus replace the MILP objective term that maximizes paralellization ($\sum_{j\in J}\, u_{\cores,j} \cdot x_j$) by the modified term $\sum_{j\in J}\, \max \{ u_{\cores,j}, 1\} \cdot x_j$ to ensure that the weight of any $x_j$ within the sum is at least~$1$.

\section{Performance}\label{sec:performance}

When executing a data analysis workflow, running time and resource usage is dominated by the executed jobs and the performance of the libraries and tools used in these.
Nevertheless, Snakemake has to process dependencies between jobs, which can incur some startup time until the actual workflow is executed.
In order to provide an estimate on the amount of time and memory needed for this computation, we took the example workflow from Figure 3 in the main manuscript and artificially inflated it by replicating the countries in the input dataset.
By this, we generated workflows of 10 to 90,000 jobs.
Then, we benchmarked runtime and memory usage of Snakemake for computing the entire graph of jobs on these on a single core of an Intel Core i5 CPU with 1.6~GHz, 8~GB RAM and a Lenovo PCIe SSD (LENSE20512GMSP34MEAT2TA) (\autoref{fig:benchmark}).
It can be seen that both runtime and memory increase linearly, starting from 0.2 seconds with 2.88 MB for 11 jobs and reaching 60 seconds with 1.1 GB for 90,000 jobs.

For future releases of Snakemake, we plan to further improve performance, for example by making use of PyPy (\url{https://www.pypy.org}), and by caching dependency resolution results between subsequent invocations of Snakemake.


\printbibliography
\end{document}
